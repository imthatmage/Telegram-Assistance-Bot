{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9eaff1-3d7c-4179-a150-2f719603aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64795249-6764-426f-bdcf-9c6811cda4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alm/miniforge3/envs/assistant_nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle \n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from typing import List, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7596f7d9-c53c-406a-8379-9ffdaec7b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stat_lm import Tokenizer, StatLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37633701-622d-454a-be27-cbd77cd8ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_name_2 = 'IlyaGusev/stihi_ru'\n",
    "\n",
    "def get_dataset(train_size: int,\n",
    "                test_size: int,\n",
    "                ds_name_1: str = 'IlyaGusev/gazeta',\n",
    "               ): \n",
    "    \n",
    "    train_dataset = load_dataset(ds_name_1, split='train')\n",
    "    test_dataset = load_dataset(ds_name_1, split='test')\n",
    "\n",
    "    train_df = pd.DataFrame(train_dataset).iloc[:train_size]\n",
    "    print(train_df.shape)\n",
    "\n",
    "    test_df = pd.DataFrame(test_dataset)[:test_size]\n",
    "    print(test_df.shape)\n",
    "\n",
    "    train_texts = (train_df['title'] + '\\n' + train_df['text']).tolist()\n",
    "    test_texts = (test_df['title'] + '\\n' + test_df['text']).tolist()\n",
    "    \n",
    "    return train_texts, test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba2a7f91-d229-48c9-a32b-b56d12af6965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5)\n",
      "(5000, 5)\n"
     ]
    }
   ],
   "source": [
    "ds_name_1 = \"IlyaGusev/gazeta\"\n",
    "train_size = 60964\n",
    "train_size = 5000\n",
    "test_size = 5000\n",
    "\n",
    "train_dataset = load_dataset(ds_name_1, split='train')\n",
    "test_dataset = load_dataset(ds_name_1, split='test')\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset).iloc[:train_size]\n",
    "print(train_df.shape)\n",
    "\n",
    "test_df = pd.DataFrame(test_dataset)[:test_size]\n",
    "print(test_df.shape)\n",
    "\n",
    "train_texts = (train_df['title'] + '\\n' + train_df['text']).tolist()\n",
    "test_texts = (test_df['title'] + '\\n' + test_df['text']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d9c2eb5-83e9-49e1-8052-41ca1781a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = train_texts + test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "69d0e5eb-d49d-496a-b8d7-01d4d7a3dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ac224e1b-6f0c-4bec-8675-19b11ab85a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "547bb00f-8e7e-4cdf-9471-77537469f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().build_vocab(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "319547b3-3f12-4cb8-8380-f75cc0a1d907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156197"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3cac9845-d306-4528-9796-396c5fb602a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example = \"В России люди любят искать приключения на голову\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "90552be8-0c55-4283-ab5e-cfcdff377145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[155768, 6797, 55871, 4855, 49768, 96698, 28478, 124855, 156194]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "50920ab8-e8f6-4392-9b85-b6fe2ccbbee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в', 'россии', 'люди', 'любят', 'искать', 'приключения', 'на', 'голову']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._tokenize(text_example, append_eos_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec382de3-5267-434d-aa5c-8c6aafec92aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в россии люди любят искать приключения на голову'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text_example), remove_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8233fc65-237c-477c-94f4-a655c4a0a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_lm = StatLM(tokenizer, context_size=4, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "08233541-1854-45c4-85a7-4dbab69e33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 730.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"обучаем\" модель - считаем статистики\n",
    "stat_lm.train(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e7af70f8-4e3e-4fb7-940d-7a0c2740e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество импортной продукции в России снизилось на 30% \n",
      "Называвший русский язык «убогим» профессор ВШЭ Гусейнов уволился \n",
      "Россияне оценили долю бесполезно потраченного на работе времени \n",
      "Школьники рассказали о тревожном состоянии после 1 сентября \n",
      "В США статую Конфедерации хотят заменить памятником Черной Пантере \n"
     ]
    }
   ],
   "source": [
    "for text in test_texts[32:37]:\n",
    "    title = text.split('\\n')[0]\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0cc3b629-22dc-49a0-8607-fa02ee0b0d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'temperature': 1.2,\n",
    "    'max_tokens': 24,\n",
    "    'sample_top_p': 0.0005,\n",
    "    'decoding_strategy': 'top-p',\n",
    "    'gen_decay': 1e-32,\n",
    "}\n",
    "\n",
    "generation_config = GenerationConfig(temperature=config['temperature'],\n",
    "                                     max_tokens=config['max_tokens'],\n",
    "                                     sample_top_p=config['sample_top_p'],\n",
    "                                     decoding_strategy=config['decoding_strategy'],\n",
    "                                     gen_decay=config['gen_decay'],\n",
    "                                     remove_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6a7cc41d-9253-477c-86c5-bb5f39da264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "за крупнейшим взломом twitter стоит 16 - летний подросток . врачи зафиксировали у него черепно намекало прикосновений благодарен анонсировано веб указывает распространят\n",
      "max tokens \n",
      "\n",
      "ni : российский гермес станет убийцей западных танков вихлянцев подкрепление an\n",
      "end of text \n",
      "\n",
      "автор хита i like to move it умер на фоне обвинений в насилии шары изучила sinoruss подрядчики аппендицита немалахов вспыхнуло аургазинском анонсировано педвузов герек\n",
      "max tokens \n",
      "\n",
      "названа опасность постоянно включенного bluetooth баширова непредумышленным надобности аппендицита вспыхнуло ютуберов немалахов давосского отторжения ландшафты перекрыта метеорологическая отменили сибирских кутаиси месседжеров колесом заливаемости transportation\n",
      "max tokens \n",
      "\n",
      "лавров сообщил о скорой встрече лукашенко с путиным в москве аппендицита smoke синодальный банкротные алтуфьеве кутаиси подкрепление масштабы проваливается легионы азамат отменили тори монополистами\n",
      "max tokens \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in test_texts[50:55]:\n",
    "    title = text.split('\\n')[0]\n",
    "    generated = stat_lm.generate_text(title, generation_config)\n",
    "    print(generated['total_text'])\n",
    "    print(generated['finish_reason'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "37e9b9da-e509-4f7b-8051-b009d02e3ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "автобус тормозит в развитии во вторник столичное правительство обсудило итоги реализации городской целевой программы развития наземного пассажирского транспорта на прошедшие три года не исключено\n",
      "max tokens \n",
      "\n",
      "реформы начнутся , когда деньги авторы и исполнители стратегии - 2010 шары давосского приговорено\n",
      "end of text \n",
      "\n",
      "зарплата превыше всего сегодня президиум высшего арбитражного суда вас рф поставил точку в матче , однако его удар оказался заблокирован . реальную возможность\n",
      "max tokens \n",
      "\n",
      "лена борется до конца во вторник на пресс - конференции в четверг . как сообщает , руководство островитян готово сделать россиянину весьма заманчивое\n",
      "max tokens \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in train_texts[21:25]:\n",
    "    title = text.split('\\n')[0]\n",
    "    generated = stat_lm.generate_text(title, generation_config)\n",
    "    print(generated['total_text'])\n",
    "    print(generated['finish_reason'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a0b630e5-a8eb-4142-a809-e06cdad02b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save('tokenizer_alm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d15ea36f-6ab9-4e1c-9973-1d90ce4e4cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_lm.save_stat('stat_lm_alm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1b3b1-6f2c-4a18-9ba1-0a26794ab18f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
